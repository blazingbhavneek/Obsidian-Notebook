
## What are Agents?

Agents are LLM-powered systems that can:

- **Reason** about which tools to use
- **Plan** a sequence of actions
- **Execute** tools to solve complex problems
- **Adapt** based on tool results

Think of an agent as an AI assistant that can use different "apps" (tools) to help you complete tasks.

---

## 1. Tool Definition

### Creating Custom Tools with @tool Decorator

The `@tool` decorator converts regular Python functions into LangChain-compatible tools that agents can use.

```python
from langchain_core.tools import tool

@tool
def add(x: float, y: float) -> float:
    """Add 'x' and 'y'."""
    return x + y

@tool
def multiply(x: float, y: float) -> float:
    """Multiply 'x' and 'y'."""
    return x * y

@tool
def exponentiate(x: float, y: float) -> float:
    """Raise 'x' to the power of 'y'."""
    return x ** y

@tool
def subtract(x: float, y: float) -> float:
    """Subtract 'x' from 'y'."""
    return y - x
```

### What Happens Behind the Scenes?

When you use `@tool`, your function becomes a `StructuredTool` object:

```python
print(add)
# Output: StructuredTool(name='add', description='Add x and y.', ...)
```

**Key Components of a Tool:**

- **Name**: Function name (e.g., "add")
- **Description**: The docstring - this is CRUCIAL for the agent to understand when to use the tool
- **Args Schema**: Defines what parameters the tool expects

---

## 2. Tool Arguments Schema

### Understanding Args Schema

Every tool has a schema that defines its expected inputs:

```python
# Get the JSON schema for the 'add' tool
add.args_schema.model_json_schema()

# Output:
{
    'properties': {
        'x': {'title': 'X', 'type': 'number'},
        'y': {'title': 'Y', 'type': 'number'}
    },
    'required': ['x', 'y'],
    'title': 'addSchema',
    'type': 'object'
}
```

**What this tells us:**

- Tool expects 2 parameters: `x` and `y`
- Both are numbers (float/int)
- Both are required
- Must be passed as an object/dictionary

### Complex Tool Schema Example

```python
exponentiate.args_schema.model_json_schema()

# Output shows the same structure but for exponentiation
{
    'properties': {
        'x': {'title': 'X', 'type': 'number'},  # base
        'y': {'title': 'Y', 'type': 'number'}   # exponent
    },
    'required': ['x', 'y'],
    'title': 'exponentiateSchema',
    'type': 'object'
}
```

---

## 3. Sample Input JSON and Tool Execution

### How Tools Receive Input from LLMs

When an agent decides to use a tool, the LLM outputs JSON that gets parsed and passed to your function:

```python
import json

# This is what the LLM outputs when it wants to use a tool
sample_tool_input = '{"x": 5, "y": 2}'

# LangChain parses this JSON into a dictionary
sample_tool_input_dict = json.loads(sample_tool_input)
print(sample_tool_input_dict)
# Output: {'x': 5, 'y': 2}

# The dictionary is unpacked as keyword arguments (**kwargs)
result = exponentiate.func(**sample_tool_input_dict)
print(result)  # Output: 25 (5^2)
```

### The Tool Execution Flow

1. **Agent decides**: "I need to calculate 5^2"
2. **LLM generates**: `'{"x": 5, "y": 2}'`
3. **LangChain parses**: `{'x': 5, 'y': 2}`
4. **Tool executes**: `exponentiate(x=5, y=2)`
5. **Result returned**: `25`

---

## 4. Creating an Agent (Tool Call Generator)

### Setting Up the Agent

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.memory import ConversationBufferMemory
from langchain.agents import create_tool_calling_agent

# Create prompt template with placeholders
prompt = ChatPromptTemplate.from_messages([
    ("system", "you're a helpful assistant"),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
    ("placeholder", "{agent_scratchpad}"),  # For agent's internal reasoning
])

# Set up memory
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)

# Create the agent
tools = [add, subtract, multiply, exponentiate]
agent = create_tool_calling_agent(
    llm=llm,
    tools=tools,
    prompt=prompt
)
```

### What Does an Agent Actually Do?

**IMPORTANT**: An agent only **plans** and **generates tool calls** - it doesn't execute them!

```python
# Agent generates a plan but doesn't execute tools
agent_output = agent.invoke({
    "input": "what is 10.7 multiplied by 7.68?",
    "chat_history": memory.chat_memory.messages,
    "intermediate_steps": []
})

print(agent_output[0])
# Output: Shows the planned tool call, not the result!
# Something like: AgentAction(tool='multiply', tool_input={'x': 10.7, 'y': 7.68})
```

### Agent vs Agent Executor

|Component|Purpose|What it does|
|---|---|---|
|**Agent**|Planning|Decides which tools to use and generates tool calls|
|**Agent Executor**|Execution|Actually runs the tools and manages the full workflow|

---

## 5. Using Agent Executor (The Complete System)

### Why We Need Agent Executor

The Agent Executor is the "brain" that:

1. Takes the agent's planned tool calls
2. Actually executes the tools
3. Feeds results back to the agent
4. Continues until the task is complete

```python
from langchain.agents import AgentExecutor

# Create the complete system
agent_executor = AgentExecutor(
    agent=agent,           # The planner
    tools=tools,          # Available tools
    memory=memory,        # Conversation memory
    verbose=True          # Show detailed execution steps
)

# Now we can actually solve problems!
result = agent_executor.invoke({
    "input": "what is 10.7 multiplied by 7.68?",
    "chat_history": memory.chat_memory.messages,
})

print(result)
# Output: Actually calculates and returns 82.176
```

### Agent Executor in Action

When you run the above code with `verbose=True`, you'll see:

```
> Entering new AgentExecutor chain...
I need to multiply 10.7 by 7.68.

Action: multiply
Action Input: {"x": 10.7, "y": 7.68}
Observation: 82.176
Thought: I have successfully calculated the multiplication.
Final Answer: 10.7 multiplied by 7.68 equals 82.176.
```

### Complex Multi-Step Example

```python
result = agent_executor.invoke({
    "input": "What is nine plus 10, minus 4 times 2, to the power of 3",
    "chat_history": memory
})

# Agent will break this down into steps:
# Step 1: 9 + 10 = 19
# Step 2: 4 * 2 = 8  
# Step 3: 19 - 8 = 11
# Step 4: 11^3 = 1331
```

---

## 6. Load_tools API for Third-Party Services

### Using Pre-built Tools

Instead of creating every tool from scratch, LangChain provides access to many pre-built tools for popular services:

```python
from langchain.agents import load_tools

# Load external service tools
toolbox = load_tools(tool_names=['serpapi'], llm=llm)
```

### Popular Third-Party Tools

```python
# Search tools
search_tools = load_tools(['serpapi', 'google-search'], llm=llm)

# Math and calculation tools  
math_tools = load_tools(['wolfram-alpha'], llm=llm)

# Web browsing tools
web_tools = load_tools(['requests'], llm=llm)

# Database tools
db_tools = load_tools(['sql-database'], llm=llm)
```

### Integration Example

```python
# Combine custom tools with third-party tools
all_tools = [add, subtract, multiply, exponentiate] + toolbox

# Create agent with enhanced capabilities
enhanced_agent = create_tool_calling_agent(
    llm=llm,
    tools=all_tools,
    prompt=prompt
)

enhanced_executor = AgentExecutor(
    agent=enhanced_agent,
    tools=all_tools,
    memory=memory,
    verbose=True
)

# Now your agent can do math AND search the web!
result = enhanced_executor.invoke({
    "input": "What's the current price of Bitcoin, and what would be the value of 1.5 Bitcoin?",
    "chat_history": memory.chat_memory.messages,
})
```

---

## Key Concepts Summary

### 1. Tool Creation Flow

```
Python Function → @tool decorator → StructuredTool → Available to Agent
```

### 2. Agent Execution Flow

```
User Input → Agent (plans) → Agent Executor (executes) → Tools → Results → Agent → Final Answer
```

### 3. Tool Input/Output Flow

```
LLM generates JSON → Parsed to dict → Unpacked as kwargs → Function executes → Result returned
```

---

## Best Practices

### Tool Design

1. **Clear docstrings**: The description is how the agent knows when to use your tool
2. **Simple parameters**: Avoid complex nested objects
3. **Meaningful names**: Tool names should be descriptive
4. **Error handling**: Tools should handle edge cases gracefully

### Agent Setup

1. **Start simple**: Begin with a few tools, add more as needed
2. **Use memory**: For multi-turn conversations
3. **Enable verbose**: To understand agent reasoning during development
4. **Test individual tools**: Before adding them to agents

### Performance Tips

1. **Limit tool count**: Too many tools can confuse the agent
2. **Specific tool descriptions**: Helps agent choose the right tool
3. **Monitor token usage**: Each tool call adds to context length
4. **Set reasonable timeouts**: For external service tools

---

## Common Patterns

### Calculator Agent

```python
# Math-focused agent
math_tools = [add, subtract, multiply, exponentiate]
calculator_agent = create_tool_calling_agent(llm, math_tools, prompt)
```

### Research Agent

```python
# Search and analysis agent  
research_tools = load_tools(['serpapi', 'wikipedia'], llm)
research_agent = create_tool_calling_agent(llm, research_tools, prompt)
```

### Hybrid Agent

```python
# Combines custom and third-party tools
hybrid_tools = math_tools + research_tools
hybrid_agent = create_tool_calling_agent(llm, hybrid_tools, prompt)
```

This architecture allows you to build powerful AI assistants that can interact with the real world through tools while maintaining conversation context through memory!