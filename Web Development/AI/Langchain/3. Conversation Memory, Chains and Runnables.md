
## What is Conversational Memory?

Conversational memory allows chatbots to remember previous interactions within a conversation. 

---

## Part 1: Traditional Memory Types (Legacy Approach)

### 1. ConversationBufferMemory

**What it does**: Stores the entire conversation history - every message is kept in memory.

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# Create memory that returns messages as ChatMessage objects
memory = ConversationBufferMemory(return_messages=True)

# Method 1: Add messages using save_context
memory.save_context(
    {"input": "Hi, my name is James"},
    {"output": "Hey James, what's up? I'm an AI model called Zeta."}
)

# Method 2: Add individual messages (alternative approach)
memory.chat_memory.add_user_message("Hi, my name is James")
memory.chat_memory.add_ai_message("Hey James, what's up? I'm Zeta.")

# Create conversation chain (wraps around LLM)
chain = ConversationChain(llm=llm, memory=memory, verbose=True)

# Test the memory
response = chain.invoke({"input": "What is my name again?"})
# Output: "Your name is James!"
```

### 2. ConversationBufferWindowMemory

**What it does**: Only keeps the last `k` messages, drops older ones.

**Pros**: Controls token usage, prevents context window overflow **Cons**: Forgets older information

```python
from langchain.memory import ConversationBufferWindowMemory

# Keep only last 4 messages (k=4)
memory = ConversationBufferWindowMemory(k=4, return_messages=True)

# Add multiple messages...
memory.chat_memory.add_user_message("Hi, my name is James")
memory.chat_memory.add_ai_message("Hey James!")
# ... add more messages ...

chain = ConversationChain(llm=llm, memory=memory, verbose=True)

# If name was in message #1 but k=4 only keeps last 4 messages:
response = chain.invoke({"input": "What is my name again?"})
# Output: "I don't recall your name" (because it was dropped!)
```

### 3. ConversationSummaryMemory

**What it does**: Instead of storing all messages, it creates and maintains a summary of the conversation.

**Pros**: Handles very long conversations, retains key information **Cons**: May lose some details in summarization, requires LLM for summarizing

```python
from langchain.memory import ConversationSummaryMemory

# Requires LLM to generate summaries
memory = ConversationSummaryMemory(llm=llm)

chain = ConversationChain(llm=llm, memory=memory, verbose=True)

# Each message updates the summary
chain.invoke({"input": "Hello, my name is James"})
chain.invoke({"input": "I'm researching conversational memory"})

# Memory now contains a summary instead of individual messages
response = chain.invoke({"input": "What is my name again?"})
# Output: "Your name is James" (from the summary)
```

### 4. ConversationSummaryBufferMemory

**What it does**: Combines summary + buffer - keeps recent messages as-is, summarizes older ones.

**Pros**: Best of both worlds - detailed recent context + summarized history **Cons**: More complex, still requires LLM for summarizing

```python
from langchain.memory import ConversationSummaryBufferMemory

# Keep recent messages up to 300 tokens, summarize the rest
memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=300,
    return_messages=True
)

chain = ConversationChain(llm=llm, memory=memory, verbose=True)

# Result: [Summary of old messages] + [Recent individual messages]
```

---

## Part 2: Modern Approach with RunnableWithMessageHistory

### Key Architectural Change

- **Old way**: Memory classes wrapped around LLM invocations
- **New way**: `RunnableWithMessageHistory` wraps around **pipelines** (prompt + LLM)

### Step 1: Basic Setup for New Approach

```python
from langchain.prompts import (
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder,
    ChatPromptTemplate
)
from langchain_core.runnables.history import RunnableWithMessageHistory

# Create prompt template with placeholder for history
prompt_template = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template("You are a helpful assistant called Zeta."),
    MessagesPlaceholder(variable_name="history"),  # This is where chat history goes
    HumanMessagePromptTemplate.from_template("{query}"),
])

# Create pipeline (prompt + LLM)
pipeline = prompt_template | llm
```

### Step 2: Rewriting ConversationBufferMemory

**Using built-in InMemoryChatMessageHistory:**

```python
from langchain_core.chat_history import InMemoryChatMessageHistory

# Session management function
chat_map = {}
def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:
    if session_id not in chat_map:
        chat_map[session_id] = InMemoryChatMessageHistory()
    return chat_map[session_id]

# Wrap pipeline with message history
pipeline_with_history = RunnableWithMessageHistory(
    pipeline,                           # Our pipeline
    get_session_history=get_chat_history, # Function to get/create history
    input_messages_key="query",         # Key for user input
    history_messages_key="history"      # Key for chat history
)

# Usage
response = pipeline_with_history.invoke(
    {"query": "Hi, my name is James"},
    config={"session_id": "user_123"}  # Each user gets their own session
)

response = pipeline_with_history.invoke(
    {"query": "What is my name again?"},
    config={"session_id": "user_123"}  # Same session = remembers previous messages
)
```

### Step 3: Rewriting ConversationBufferWindowMemory

**Creating custom memory class for window functionality:**

```python
from pydantic import BaseModel, Field
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.messages import BaseMessage

# Custom class that inherits from BaseChatMessageHistory
class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):
    messages: list[BaseMessage] = Field(default_factory=list)
    k: int = Field(default_factory=int)

    def __init__(self, k: int):
        super().__init__(k=k)

    def add_messages(self, messages: list[BaseMessage]) -> None:
        """Add messages and keep only last k messages"""
        self.messages.extend(messages)
        self.messages = self.messages[-self.k:]  # Keep only last k messages

    def clear(self) -> None:
        self.messages = []

# Updated session management with configurable k
def get_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:
    if session_id not in chat_map:
        chat_map[session_id] = BufferWindowMessageHistory(k=k)
    return chat_map[session_id]

# Pipeline with configurable parameters
from langchain_core.runnables import ConfigurableFieldSpec

pipeline_with_history = RunnableWithMessageHistory(
    pipeline,
    get_session_history=get_chat_history,
    input_messages_key="query",
    history_messages_key="history",
    history_factory_config=[  # Allow runtime configuration
        ConfigurableFieldSpec(
            id="session_id",
            annotation=str,
            name="Session ID",
            default="default_session"
        ),
        ConfigurableFieldSpec(
            id="k",
            annotation=int,
            name="k",
            description="Number of messages to keep",
            default=4
        )
    ]
)

# Usage with custom k value
response = pipeline_with_history.invoke(
    {"query": "Hi, my name is James"},
    config={"configurable": {"session_id": "user_123", "k": 6}}
)
```

### Step 4: Rewriting ConversationSummaryMemory

**Creating custom class that summarizes messages:**

```python
from langchain_core.messages import SystemMessage

class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):
    messages: list[BaseMessage] = Field(default_factory=list)
    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)

    def __init__(self, llm: ChatOpenAI):
        super().__init__(llm=llm)

    def add_messages(self, messages: list[BaseMessage]) -> None:
        """Add messages and create/update summary"""
        self.messages.extend(messages)
        
        # Create summary prompt
        summary_prompt = ChatPromptTemplate.from_messages([
            SystemMessagePromptTemplate.from_template(
                "Summarize this conversation, maintaining key information:"
            ),
            HumanMessagePromptTemplate.from_template(
                "Existing summary: {existing_summary}\n"
                "New messages: {messages}"
            )
        ])
        
        # Get existing summary (if any)
        existing_summary = self.messages[0].content if self.messages else ""
        
        # Generate new summary
        new_summary = self.llm.invoke(
            summary_prompt.format_messages(
                existing_summary=existing_summary,
                messages=[msg.content for msg in messages]
            )
        )
        
        # Replace all messages with single summary message
        self.messages = [SystemMessage(content=new_summary.content)]

    def clear(self) -> None:
        self.messages = []

# Usage is similar to buffer window, but with llm parameter
```

### Step 5: Rewriting ConversationSummaryBufferMemory

**Most complex: combines buffer + summary:**

```python
class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):
    messages: list[BaseMessage] = Field(default_factory=list)
    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)
    k: int = Field(default_factory=int)

    def __init__(self, llm: ChatOpenAI, k: int):
        super().__init__(llm=llm, k=k)

    def add_messages(self, messages: list[BaseMessage]) -> None:
        """Add messages, keep last k, summarize the rest"""
        
        # Check if we already have a summary
        existing_summary = None
        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):
            existing_summary = self.messages.pop(0)  # Remove existing summary
        
        # Add new messages
        self.messages.extend(messages)
        
        # If we have too many messages, summarize the old ones
        if len(self.messages) > self.k:
            # Split messages: old ones to summarize, recent ones to keep
            old_messages = self.messages[:-self.k]
            self.messages = self.messages[-self.k:]  # Keep only recent ones
            
            # Create summary of old messages
            # ... (summarization logic similar to ConversationSummaryMemory)
            
            # Prepend summary to recent messages
            self.messages = [summary_message] + self.messages

    def clear(self) -> None:
        self.messages = []
```

---

## Key Differences: Old vs New Approach

|Aspect|Old Approach|New Approach|
|---|---|---|
|**Architecture**|Memory wraps around LLM|RunnableWithMessageHistory wraps around pipeline|
|**Flexibility**|Fixed memory types|Custom memory classes, configurable parameters|
|**Session Management**|Built into chains|Explicit session management with get_session_history|
|**Extensibility**|Limited customization|Full control over memory behavior|
|**Future-proofing**|Being deprecated|Current recommended approach|

## When to Use Which Memory Type

1. **ConversationBufferMemory**: Short conversations, need full context
2. **ConversationBufferWindowMemory**: Medium conversations, want to control token usage
3. **ConversationSummaryMemory**: Very long conversations, key information more important than exact wording
4. **ConversationSummaryBufferMemory**: Long conversations where you need both recent detail and historical context

## Best Practices

1. **Always use `return_messages=True`** with chat models
2. **Choose memory type based on conversation length and importance of details**
3. **Use session IDs** to separate different users/conversations
4. **Monitor token usage** - memory directly affects cost and latency
5. **Test memory behavior** with your specific use case - summarization might lose important details

## Common Pitfalls

1. **Forgetting session management** - each conversation needs its own session ID
2. **Not accounting for token limits** - long conversations will eventually hit context windows
3. **Over-summarizing** - important details might get lost in summaries
4. **Under-summarizing** - keeping too much history increases costs and latency