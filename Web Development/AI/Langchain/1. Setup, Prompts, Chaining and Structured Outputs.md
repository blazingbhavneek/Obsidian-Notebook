When to use langchain
- Langchain is framework for abstracting LLM operations
- ![[Pasted image 20250528093356.png]] Operations like this


## 1. Setting Up Ollama in Google Colab

### What is Ollama?

Ollama is a tool that lets you run large language models locally on your computer. In Google Colab, we'll install and use it to run AI models without needing external API keys.

### Step-by-Step Installation

```python
# Step 1: Install terminal extension for Colab
!pip install colab-xterm
%load_ext colabxterm

# Step 2: Download and install Ollama
!curl https://ollama.ai/install.sh | sh

# Step 3: Open terminal (run this in a separate cell)
%xterm
# In the terminal, type: ollama serve
# Keep this running in the background

# Step 4: Download a small AI model (Gemma 1B parameters)
!ollama pull gemma3:1b
model_name = "gemma3:1b"

# Step 5: Check what models are installed
!ollama list

# Step 6: Install required Python packages
!pip uninstall -y langchain langchain-core langchain-ollama
!pip install -U ollama
!pip install \
  langchain==0.3.25 \
  langchain-core==0.3.58 \
  langchain-ollama==0.3.0

# Step 7: Create the LLM object
from langchain_ollama import ChatOllama
import re

llm = ChatOllama(
    model=model_name,
    temperature=0,  # 0 = deterministic, 1 = creative
)
```

**Key Points:**

- `temperature=0` makes responses consistent and predictable
- `ollama serve` must be running in the background
- Gemma3:1b is a lightweight model good for learning

---

## 2. LangChain Prompt Templates and Chains

### Understanding Message Templates

LangChain uses different types of message templates to structure conversations with AI:

#### SystemMessagePromptTemplate

- Sets the AI's role and behavior
- Like giving the AI a job description

```python
from langchain.prompts import SystemMessagePromptTemplate

system_prompt = SystemMessagePromptTemplate.from_template(
    "You are an AI assistant that helps generate article titles."
)
```

#### HumanMessagePromptTemplate

- Represents what the user says to the AI
- Can include variables using `{variable_name}`

```python
from langchain.prompts import HumanMessagePromptTemplate

user_prompt = HumanMessagePromptTemplate.from_template(
    """You are tasked with creating a name for a article.
The article is here for you to examine {article}
The name should be based of the context of the article.
Be creative, but make sure the names are clear, catchy,
and relevant to the theme of the article.
Only output the article name, no other explanation or
text can be provided.""",
    input_variables=["article"]  # List of variables this template uses
)

# Test how variables work
formatted_prompt = user_prompt.format(article="TEST STRING")
print(formatted_prompt)
```

#### ChatPromptTemplate

- Combines system and user messages into a complete conversation

```python
from langchain.prompts import ChatPromptTemplate

# Combine system and user prompts
first_prompt = ChatPromptTemplate.from_messages([system_prompt, user_prompt])

# See the complete formatted prompt
print(first_prompt.format(article="TEST STRING"))
```

### Variables in Templates

Variables are placeholders that get replaced with actual content:

- Use `{variable_name}` syntax
- Declare them in `input_variables=["variable_name"]`
- Fill them using `.format(variable_name="actual value")`

### Creating More Complex Prompts

```python
# A different user prompt for editing articles
third_user_prompt = HumanMessagePromptTemplate.from_template(
    """You are tasked with creating a new paragraph for the
article. The article is here for you to examine:
---
{article}
---
Choose one paragraph to review and edit. During your edit
ensure you provide constructive feedback to the user so they
can learn where to improve their own writing.""",
    input_variables=["article"]
)

# Create a new chat prompt
third_prompt = ChatPromptTemplate.from_messages([
    system_prompt,
    third_user_prompt
])
```

---

## 3. Structured Output with Pydantic

### What is Pydantic?

Pydantic helps us define the exact structure we want from AI responses, instead of getting random text.

### Creating a Data Model

```python
from pydantic import BaseModel, Field

class Paragraph(BaseModel):
    original_paragraph: str = Field(description="The original paragraph")
    edited_paragraph: str = Field(description="The improved edited paragraph")
    feedback: str = Field(description=(
        "Constructive feedback on the original paragraph"
    ))
```

**Key Components:**

- `BaseModel`: The foundation class for data structures
- `Field()`: Adds descriptions to help the AI understand what each field should contain
- Type hints (`str`): Tell Python what type of data each field should be

### Making the LLM Return Structured Data

```python
# Transform our LLM to return structured output
structured_llm = llm.with_structured_output(Paragraph)
```

---

## 4. LangChain Chains and LCEL

### What are Chains?

Chains connect different steps together to create a workflow. Think of it like a pipeline where data flows from one step to the next.

### LCEL (LangChain Expression Language)

LCEL uses the `|` (pipe) operator to chain operations together, similar to Unix pipes.

### Building a Complete Chain

```python
chain_three = (
    {"article": lambda x: x["article"]}  # Step 1: Extract article from input
    | third_prompt                       # Step 2: Format the prompt
    | structured_llm                     # Step 3: Get AI response
    | {                                  # Step 4: Extract specific fields
        "original_paragraph": lambda x: x.original_paragraph,
        "edited_paragraph": lambda x: x.edited_paragraph,
        "feedback": lambda x: x.feedback
    }
)
```


### Running the Chain

```python
# Assuming you have an article variable defined
article = "Your article text here..."

# Run the entire chain
out = chain_three.invoke({"article": article})

# Access the results
print("Original:", out["original_paragraph"])
print("Edited:", out["edited_paragraph"])
print("Feedback:", out["feedback"])
```
