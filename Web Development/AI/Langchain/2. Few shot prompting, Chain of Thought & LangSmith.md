

## 1. Ways to Input to ChatPromptTemplate

### Method 1: Simple Tuple Format

The simplest way to create prompts is using tuples with role names:

```python
prompt = """
Answer the user's query based on the context below.
If you cannot answer the question using the
provided information answer with "I don't know".

Context: {context}
"""

from langchain.prompts import ChatPromptTemplate

# Using simple tuple format (role, message)
prompt_template = ChatPromptTemplate.from_messages([
    ("system", prompt),           # System message
    ("user", "{query}"),         # User message with variable
])
```

### Method 2: Explicit Template Objects

More verbose but gives you more control:

```python
from langchain.prompts import (
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate
)

prompt_template = ChatPromptTemplate.from_messages([
    SystemMessagePromptTemplate.from_template(prompt),
    HumanMessagePromptTemplate.from_template("{query}"),
])
```


---

## 2. Understanding input_variables

### Automatic Variable Detection

ChatPromptTemplate automatically detects variables in your templates:

```python
# Check what variables the template expects
print(prompt_template.input_variables)
# Output: ['context', 'query']
```

**How it works:**

- LangChain scans your template for `{variable_name}` patterns
- Automatically creates the `input_variables` list
- You don't need to manually specify them (unlike individual message templates)

### Variable Requirements

- All variables in `{}` must be provided when invoking
- Missing variables will cause errors
- Extra variables are ignored

---

## 3. Template Attributes

### Key Attributes to Know

```python
# See the input variables automatically detected
prompt_template.input_variables
# Output: ['context', 'query']

# See the actual message templates
prompt_template.messages
# Output: [SystemMessagePromptTemplate(...), HumanMessagePromptTemplate(...)]
```

### Understanding messages Attribute

The `messages` attribute contains the actual template objects:

- Each message has its own type (System, Human, AI)
- Each can have different formatting rules
- Useful for debugging and understanding your prompt structure

---

## 4. Automatic Variable Fitting

### Direct Variable Passing

You can pass variables directly to the prompt and LangChain will fit them automatically:

```python
# Create a pipeline that handles variable mapping
pipeline = (
    {
        "query": lambda x: x["query"],      # Extract query from input
        "context": lambda x: x["context"]   # Extract context from input
    }
    | prompt_template  # Variables automatically fit into template
    | llm             # Send to language model
)

# Example data
context = """Aurelio AI is an AI company developing tooling for AI
engineers. Their focus is on language AI with the team having strong
expertise in building AI agents and a strong background in
information retrieval.

The company is behind several open source frameworks, most notably
Semantic Router and Semantic Chunkers. They also have an AI
Platform providing engineers with tooling to help them build with
AI. Finally, the team also provides development services to other
organizations to help them bring their AI tech to market.

Aurelio AI became LangChain Experts in September 2024 after a long
track record of delivering AI solutions built with the LangChain
ecosystem."""

query = "what does Aurelio AI do?"

# Invoke the pipeline - variables automatically fitted
result = pipeline.invoke({"query": query, "context": context})
```

**Key Points:**

- The dictionary keys must match your template variables exactly
- LangChain handles the variable substitution automatically
- No need to manually call `.format()`

---

## 5. Few-Shot Example Prompts

### What are Few-Shot Prompts?

Few-shot prompts show the AI examples of input-output pairs to help it understand the desired response format and style.

### Creating Example Templates

```python
# Define how each example should be formatted
example_prompt = ChatPromptTemplate.from_messages([
    ("human", "{input}"),    # What the human says
    ("ai", "{output}"),      # What the AI should respond
])

# Create your examples
examples = [
    {"input": "Here is query #1", "output": "Here is the answer #1"},
    {"input": "Here is query #2", "output": "Here is the answer #2"},
    {"input": "Here is query #3", "output": "Here is the answer #3"},
]
```

### Building Few-Shot Prompt Template

```python
from langchain.prompts import FewShotChatMessagePromptTemplate

few_shot_prompt = FewShotChatMessagePromptTemplate(
    example_prompt=example_prompt,  # How to format each example
    examples=examples,              # The actual examples
)

# See how it looks when formatted
print(few_shot_prompt.format())
```

### Integrating with ChatPromptTemplate

```python
# Combine system prompt, examples, and user query
prompt_template = ChatPromptTemplate.from_messages([
    ("system", new_system_prompt),  # System instructions
    few_shot_prompt,                # Examples go in the middle
    ("user", "{query}"),            # User's actual question
])

# Create pipeline
pipeline = prompt_template | llm
result = pipeline.invoke({"query": query, "context": context}).content
```

### Benefits of Few-Shot Prompts

1. **Consistency**: AI learns the exact format you want
2. **Quality**: Examples guide the AI toward better responses
3. **Reliability**: Reduces random or unexpected outputs
4. **Training**: Works like mini-training examples

---

## 6. Chain-of-Thought Prompting

### What is Chain-of-Thought?

Chain-of-thought prompting encourages the AI to break down complex problems into steps, leading to more accurate and explainable answers.

### Standard Prompt (No Chain-of-Thought)

```python
no_cot_system_prompt = """
Be a helpful assistant and answer the user's question.

You MUST answer the question directly without any other
text or explanation.
"""

no_cot_prompt_template = ChatPromptTemplate.from_messages([
    ("system", no_cot_system_prompt),
    ("user", "{query}"),
])
```

### Chain-of-Thought Prompt

```python
cot_system_prompt = """
Be a helpful assistant and answer the user's question.

To answer the question, you must:

- List systematically and in precise detail all
  subproblems that need to be solved to answer the
  question.
- Solve each sub problem INDIVIDUALLY and in sequence.
- Finally, use everything you have worked through to
  provide the final answer.
"""

cot_prompt_template = ChatPromptTemplate.from_messages([
    ("system", cot_system_prompt),
    ("user", "{query}"),
])
```


---

## 7. LangSmith Integration for Monitoring and Debugging

### What is LangSmith?

LangSmith is a monitoring and debugging platform for LangChain applications. It helps you:

- Track all your LLM calls and chains
- Debug issues in your AI applications
- Monitor performance and costs
- Analyze conversation flows

### Setting Up LangSmith

#### Step 1: Installation (Already Done)

```python
# LangSmith comes with LangChain, no additional installation needed
# But ensure you have the correct versions:
!pip install \
  langchain==0.3.25 \
  langchain-core==0.3.58 \
  langchain-ollama==0.3.0
```

#### Step 2: Configuration

```python
import os
from getpass import getpass

# Enable LangSmith tracing
os.environ["LANGSMITH_TRACING"] = "true"
os.environ["LANGSMITH_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGSMITH_API_KEY"] = getpass("Enter LangSmith API Key: ")
os.environ["LANGSMITH_PROJECT"] = "langchain-tut"  # Your project name
```

**Environment Variables Explained:**

- `LANGSMITH_TRACING`: Enables/disables tracing (`"true"` or `"false"`)
- `LANGSMITH_ENDPOINT`: The LangSmith API endpoint (usually the default)
- `LANGSMITH_API_KEY`: Your personal API key from LangSmith dashboard
- `LANGSMITH_PROJECT`: Project name to organize your traces

### Getting Your LangSmith API Key

1. Go to [smith.langchain.com](https://smith.langchain.com)
2. Sign up or log in
3. Navigate to Settings â†’ API Keys
4. Create a new API key
5. Copy the key for use in `getpass()`

### Basic Usage with Automatic Tracing

Once configured, all your LangChain operations are automatically traced:

```python
from langchain_ollama import ChatOllama

llm = ChatOllama(
    model=model_name,
    temperature=0,
)

# This call will automatically be traced in LangSmith
response = llm.invoke("Hello, world!")
print(response.content)
```

**What gets tracked automatically:**

- Input prompts
- Model responses
- Execution time
- Token usage (if available)
- Error messages
- Chain execution steps

### Custom Tracing with @traceable

For custom functions that aren't part of LangChain, use the `@traceable` decorator:

```python
from langsmith import traceable
import random

@traceable
def random_error():
    """Example function that randomly throws errors"""
    number = random.randint(0, 1)
    if number == 0:
        raise ValueError("Random error")
    else:
        return "No error"

# This function will now be traced in LangSmith
try:
    result = random_error()
    print(f"Result: {result}")
except ValueError as e:
    print(f"Error occurred: {e}")
```

### Tracing LangChain Chains

Your existing chains will automatically be traced:

```python
# This entire pipeline will be traced step by step
pipeline = (
    {
        "query": lambda x: x["query"],
        "context": lambda x: x["context"]
    }
    | prompt_template
    | llm
)

# Each step appears as a separate trace in LangSmith
result = pipeline.invoke({"query": query, "context": context})
```
