
## 1. LangChain Output Parser

### What is an Output Parser?

Output parsers convert the raw LLM response (usually an `AIMessage` object) into a more usable format like strings, JSON, or structured data.

### StrOutputParser - Most Common Parser

```python
from langchain.schema.output_parser import StrOutputParser

# LLM returns an AIMessage object
llm_out = llm.invoke("Hello there")
print(type(llm_out))  # <class 'langchain_core.messages.ai.AIMessage'>

# Parser converts it to a simple string
output_parser = StrOutputParser()
parsed_output = output_parser.invoke(llm_out)
print(type(parsed_output))  # <class 'str'>
print(parsed_output)        # "Hello! How can I help you today?"
```

### Using in LCEL Chain

```python
from langchain import PromptTemplate

prompt = PromptTemplate(
    input_variables=["topic"],
    template="Give me a small report on {topic}"
)

# Traditional way (deprecated)
chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)

# LCEL way (recommended)
lcel_chain = prompt | llm | output_parser
result = lcel_chain.invoke({"topic": "AI"})  # Returns clean string
```

**Key Benefits:**

- Converts complex LLM objects to simple formats
- Essential for clean data flow in chains
- Makes output easier to work with in downstream processes

## 2. RunnableLambda and Function Chaining

### What is RunnableLambda?

`RunnableLambda` wraps regular Python functions to make them compatible with LCEL chaining using the pipe operator `|`.

### Basic Function Wrapping

```python
from langchain_core.runnables import RunnableLambda

# Regular Python functions
def add_five(x):
    return x + 5

def multiply_two(x):
    return x * 2

# Wrap functions to make them chainable
add_runnable = RunnableLambda(add_five)
mul_runnable = RunnableLambda(multiply_two)

# Chain them together
chain = add_runnable | mul_runnable
result = chain.invoke(3)  # (3 + 5) * 2 = 16
```

### Understanding the Pipe Operator `|`

The pipe operator creates a flow where the output of the left side becomes the input of the right side:

```python
# This flow: input → add_five → multiply_two → output
chain = add_runnable | mul_runnable
# Equivalent to: multiply_two(add_five(input))
```

### Text Processing Example

```python
def extract_main_content(text):
    """Extract content after the first paragraph"""
    if "\n\n" in text:
        return "\n".join(text.split("\n\n")[1:])
    return text

def replace_word(text):
    """Replace specific words in text"""
    return text.replace("AI", "Machine Learning")

# Create runnables
extract_runnable = RunnableLambda(extract_main_content)
replace_runnable = RunnableLambda(replace_word)

# Complete chain: prompt → LLM → parse → extract → replace
full_chain = prompt | llm | output_parser | extract_runnable | replace_runnable
result = full_chain.invoke({"topic": "artificial intelligence"})
```

**Important Notes:**

- Functions must accept exactly ONE argument
- For multiple inputs, use dictionaries and unpack inside the function
- All LangChain components (prompts, LLMs, parsers) are already "runnable"

## 3. Open Source Vector Databases

### Using Chroma (Local Vector Database)

```python
# Install: pip install chromadb
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings

# Use open-source embedding model
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# Create vector store
documents = [
    "LangChain is a framework for building LLM applications",
    "Vector databases store embeddings for similarity search",
    "LCEL uses pipe operators to chain components together"
]

vectorstore = Chroma.from_texts(
    texts=documents,
    embedding=embedding_model,
    persist_directory="./chroma_db"  # Saves to disk
)
```

### Using FAISS (Facebook AI Similarity Search)

```python
# Install: pip install faiss-cpu
from langchain.vectorstores import FAISS

# Create FAISS vector store
vectorstore = FAISS.from_texts(
    texts=documents,
    embedding=embedding_model
)

# Save and load
vectorstore.save_local("faiss_index")
loaded_vectorstore = FAISS.load_local("faiss_index", embedding_model)
```

### Using Qdrant (Open Source)

```python
# Install: pip install qdrant-client
from langchain.vectorstores import Qdrant

vectorstore = Qdrant.from_texts(
    texts=documents,
    embedding=embedding_model,
    location=":memory:",  # In-memory for testing
    collection_name="my_documents"
)
```

**Comparison:**

- **Chroma**: Best for local development, persistent storage
- **FAISS**: Fastest for similarity search, good for large datasets
- **Qdrant**: Production-ready, supports filtering and metadata

## 4. Normal Retrieval Chain using LCEL

### Setting Up Basic RAG Chain

```python
from langchain.prompts import ChatPromptTemplate

# Create vector store with your documents
documents = [
    "Python is a programming language used for AI development",
    "LangChain helps build applications with large language models",
    "Vector databases enable semantic search capabilities"
]

vectorstore = Chroma.from_texts(
    texts=documents,
    embedding=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
)

# Convert to retriever
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 2}  # Return top 2 most relevant documents
)

# Create RAG prompt
rag_prompt = ChatPromptTemplate.from_template("""
Use the following context to answer the question:

Context: {context}

Question: {question}

Answer based only on the provided context.
""")
```

### Simple Retrieval Chain

```python
from langchain_core.runnables import RunnablePassthrough

def format_docs(docs):
    """Format retrieved documents into a single string"""
    return "\n\n".join([doc.page_content for doc in docs])

# Create the RAG chain
rag_chain = (
    {
        "context": retriever | RunnableLambda(format_docs),
        "question": RunnablePassthrough()
    }
    | rag_prompt
    | llm
    | output_parser
)

# Use the chain
result = rag_chain.invoke("What is LangChain used for?")
print(result)
```

### How It Works:

1. **Question** goes to retriever to find relevant documents
2. **Documents** are formatted into context string
3. **Question** passes through unchanged (RunnablePassthrough)
4. **Prompt** combines context and question
5. **LLM** generates answer based on context
6. **Parser** converts to clean string output

## 5. RunnablePassthrough and RunnableParallel

### RunnablePassthrough

Passes input through unchanged - useful when you need the same input in multiple places:

```python
from langchain_core.runnables import RunnablePassthrough

# Example: Need both original question and processed version
def process_question(question):
    return question.upper()

chain = {
    "original": RunnablePassthrough(),           # Passes through unchanged
    "processed": RunnableLambda(process_question) # Processes the input
}

result = chain.invoke("what is AI?")
# Output: {"original": "what is AI?", "processed": "WHAT IS AI?"}
```

### RunnableParallel

Runs multiple operations in parallel on the same input:

```python
from langchain_core.runnables import RunnableParallel

def get_word_count(text):
    return len(text.split())

def get_char_count(text):
    return len(text)

# Run both functions in parallel
parallel_analysis = RunnableParallel({
    "word_count": RunnableLambda(get_word_count),
    "char_count": RunnableLambda(get_char_count),
    "original": RunnablePassthrough()
})

result = parallel_analysis.invoke("Hello world this is a test")
# Output: {"word_count": 6, "char_count": 27, "original": "Hello world..."}
```

### Dictionary Syntax

Both can be used with dictionary syntax for cleaner code:

```python
# These are equivalent
parallel_dict = {
    "context": retriever,
    "question": RunnablePassthrough()
}

parallel_explicit = RunnableParallel({
    "context": retriever,
    "question": RunnablePassthrough()
})
```

## 6. Parallel Retrieval Chain with Two Vector Stores

### Setting Up Multiple Vector Stores

```python
# Create two specialized knowledge bases
vectorstore_tech = Chroma.from_texts([
    "Python is used for backend development and AI",
    "JavaScript runs in browsers and Node.js servers",
    "Docker containers package applications with dependencies"
], embedding=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2"))

vectorstore_business = Chroma.from_texts([
    "Agile methodology emphasizes iterative development",
    "Market research helps identify customer needs",
    "ROI measures return on investment for projects"
], embedding=HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2"))

# Convert to retrievers
tech_retriever = vectorstore_tech.as_retriever(search_kwargs={"k": 2})
business_retriever = vectorstore_business.as_retriever(search_kwargs={"k": 2})
```

### Creating Parallel Retrieval Chain

```python
def format_docs(docs):
    """Format documents into readable string"""
    return "\n".join([f"- {doc.page_content}" for doc in docs])

# Parallel retrieval setup
parallel_retrieval = RunnableParallel({
    "tech_context": tech_retriever | RunnableLambda(format_docs),
    "business_context": business_retriever | RunnableLambda(format_docs),
    "question": RunnablePassthrough()
})

# Multi-context prompt
multi_context_prompt = ChatPromptTemplate.from_template("""
Answer the question using information from both technical and business contexts:

Technical Context:
{tech_context}

Business Context:
{business_context}

Question: {question}

Provide a comprehensive answer that combines insights from both contexts.
""")

# Complete parallel RAG chain
parallel_rag_chain = (
    parallel_retrieval
    | multi_context_prompt
    | llm
    | output_parser
)
```

### Using the Parallel Chain

```python
result = parallel_rag_chain.invoke(
    "How can Python development fit into an Agile workflow?"
)
print(result)
```

### Chain Flow Visualization

```
Input Question
     |
     v
┌─────────────────────────┐
│  RunnableParallel       │
│  ┌─────────────────────┐│
│  │ tech_retriever      ││ ─── Technical docs
│  │ business_retriever  ││ ─── Business docs  
│  │ RunnablePassthrough ││ ─── Original question
│  └─────────────────────┘│
└─────────────────────────┘
     |
     v
Multi-Context Prompt
     |
     v
LLM Generation
     |
     v
String Output
```

### Benefits of Parallel Retrieval:

- **Comprehensive answers**: Combines information from multiple knowledge domains
- **Efficiency**: Retrievals happen simultaneously, not sequentially
- **Modularity**: Easy to add/remove knowledge sources
- **Flexibility**: Each retriever can have different search parameters

### Advanced Example with Different Search Types:

```python
# Different retrievers for different purposes
recent_retriever = vectorstore_tech.as_retriever(
    search_kwargs={"k": 1, "filter": {"date": "2024"}}
)
general_retriever = vectorstore_business.as_retriever(
    search_kwargs={"k": 3}
)

specialized_parallel = RunnableParallel({
    "recent_tech": recent_retriever | RunnableLambda(format_docs),
    "general_business": general_retriever | RunnableLambda(format_docs),
    "query": RunnablePassthrough()
})
```

This approach allows you to create sophisticated RAG systems that can pull from multiple specialized knowledge bases simultaneously, providing richer and more comprehensive responses to user queries.